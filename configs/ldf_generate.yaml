exp_name: ldf_generate # Experiment names
seed: 1234 # Seed value
debug: false
train: true

save_dir: ./outputs
resume_ckpt: null
test_ckpt: "${dirs.outputs}/20251107_021814_ldf_stream/step_step=240000_converted.ckpt"
test_vae_ckpt: "${dirs.outputs}/vae_1d_z4_step=300000.ckpt"
test_vae:
    target: models.vae_wan_1d.VAEWanModel
    ema_decay: 0.99
    params:
        input_dim: 263
        z_dim: 4
test_setting:
    render: true
    simple: true
    recover_dim: 263
val_repeat: 1

logger:
    wandb:
        wandb_key: "${wandb_info.key}"
        project: "${wandb_info.project}"
        entity: "${wandb_info.entity}"

trainer: # lightning trainer
    max_steps: 300000
    accelerator: gpu
    devices: [0]
    log_every_n_steps: 100
    precision: bf16-mixed
validation:
    validation_steps: 10000
    test_steps: 10000
    save_every_n_steps: 10000
    save_top_k: 100
metrics:
    dim: 263
    t2m:
        evaluate_text: true
        fid_target: "original" # vae or original
        metric_mean_path: "${dirs.deps}/t2m/meta/mean.npy"
        metric_std_path: "${dirs.deps}/t2m/meta/std.npy"
        wordvectorizer:
            target: metrics.tools.word_vectorizer.WordVectorizer
            params:
                meta_root: "${dirs.deps}/glove"
                prefix: our_vab
            max_text_len: 20
        textencoder:
            target: metrics.tools.t2m_evaluator.TextEncoderBiGRUCo
            ckpt: "${dirs.deps}/t2m/humanml3d/text_encoder.pt"
            params:
                word_size: 300
                pos_size: 15
                hidden_size: 512
                output_size: 512
        moveencoder:
            target: metrics.tools.t2m_evaluator.MovementConvEncoder
            ckpt: "${dirs.deps}/t2m/humanml3d/movement_encoder.pt"
            params:
                input_size: 259
                hidden_size: 512
                output_size: 512
        motionencoder:
            target: metrics.tools.t2m_evaluator.MotionEncoderBiGRUCo
            ckpt: "${dirs.deps}/t2m/humanml3d/motion_encoder.pt"
            params:
                input_size: 512
                hidden_size: 1024
                output_size: 512

data:
    target: datasets.multi.MultiDataset
    collate_fn: datasets.multi.collate_fn
    train_bs: 4
    val_bs: 4
    test_bs: 1
    num_workers: 8
    datasets:
        - target: datasets.generate.GenerateDataset
          num_samples: 100
          dim: 263
          token_dim: 4
          feature_fps: 20
          token_fps: 5

model:
    target: models.diffusion_forcing_wan.DiffForcingWanModel
    ema_decay: 0.99
    params:
        input_dim: 4
        noise_steps: 10
        crossmodules:
            - name: T5TextCrossModule
              len: 512
              dim: 4096
              checkpoint_path: "${dirs.deps}/t5_umt5-xxl-enc-bf16/models_t5_umt5-xxl-enc-bf16.pth"
              tokenizer_path: "${dirs.deps}/t5_umt5-xxl-enc-bf16/google/umt5-xxl"

optimizer:
    target: AdamW
    params:
        lr: 2e-4
        betas: [0.9, 0.99]
        weight_decay: 0.0
        eps: 1e-8

# lr_scheduler:
#     target: diffusers.optimization.get_constant_schedule_with_warmup
#     params:
#         num_warmup_steps: 1000

lr_scheduler:
    target: CosineAnnealingLR
    params:
        T_max: 1000
        eta_min: 1e-6
